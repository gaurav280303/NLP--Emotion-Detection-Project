# -*- coding: utf-8 -*-
"""NLP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12PhjQ_XnYuLRP6VxDyb9NrIdsWrJL5FY
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv('train.txt', sep= ";", header= None, names=['text', 'emotion'])
df.head()

df.isnull().sum()

unique_emotion=df['emotion'].unique()

"""Conerting output( emotions) in numbers"""

emotion_numbers = {}
i=0
for emo in unique_emotion:
  emotion_numbers[emo]=i
  i+=1

df['emotion']=df['emotion'].astype(str).str.strip().map(emotion_numbers)
df.head()

"""df['emotion'].astype(str): Ensures that all values in the 'emotion' column are treated as strings.
.str.strip(): Removes any leading or trailing whitespace from the emotion strings (e.g., ' sadness ' becomes 'sadness').
.map(emotion_numbers): This is a pandas Series method that uses the emotion_numbers dictionary to replace each emotion string in the 'emotion' column with its corresponding numerical value. If an emotion string is not found in the dictionary (which shouldn't happen here if unique_emotion was derived from the column), it would result in NaN.
df.head(): Finally, this displays the first few rows of the DataFrame df to show the updated 'emotion' column with numerical values
"""



"""I.Conerting text in lowecase"""

df['text']=df['text'].apply(lambda x: x.lower())

"""II. Removing punctuations"""

import string
def remove_punc(txt):
  return txt.translate(str.maketrans(",",string.punctuation))

"""Let's break down the remove_punc function defined in the cell Nd2Nf5ifgyFZ:

import string: This line imports Python's built-in string module. This module contains a collection of common string constants, including string.punctuation, which is a string containing all ASCII punctuation characters (e.g., ! " # $ % & ' ( ) * + , - . / : ; < = > ? @ [ \ ] ^ _ ` { | } ~).

def remove_punc(txt):: This defines a Python function named remove_punc that takes one argument, txt, which is expected to be a string.

return txt.translate(str.maketrans("", string.punctuation)): This is the core of the function and does the actual punctuation removal:

str.maketrans("", string.punctuation): This is a static method of the str class that creates a translation table. It takes three arguments (though the third is often omitted for simple deletion). In this case:
The first argument "" (an empty string) indicates that no characters should be mapped to other characters (i.e., no replacements).
The second argument string.punctuation specifies the characters to be deleted. Any character present in string.punctuation will be deleted from the input string txt.
txt.translate(...): This is a string method that applies the translation table created by str.maketrans(). For each character in the input string txt, if it's found in the deletion set specified by the translation table (i.e., if it's a punctuation mark), it will be removed. Characters not in the deletion set are kept as they are.
"""

import string
df['text']=df['text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))

"""III. Remove Numbers"""

def remove_num(txt):
  new=""
  for i in txt:
    if not i.isdigit():
      new=new+i
  return new

df['text']=df['text'].apply(remove_num)

"""def remove_num(txt):: This line defines a Python function named remove_num that takes one argument, txt, which is expected to be a string.

new="": Inside the function, an empty string named new is initialized. This string will be used to build up the new text without numbers.

for i in txt:: This starts a for loop that iterates through each character (i) in the input string txt.

if not i.isdigit():: Inside the loop, this if statement checks if the current character i is not a digit. The .isdigit() method returns True if all characters in the string are digits and there is at least one character, False otherwise.

new=new+i: If the character i is not a digit, it is appended to the new string.

return new: After checking all characters in the input txt string, the function returns the new string, which now contains only the non-digit characters from the original text.

df['text']=df['text'].apply(remove_num): This line applies the remove_num function to every entry in the 'text' column of your DataFrame df. For each text entry, the remove_num function is called, and its returned value (the text without numbers) then replaces the original text in that specific row of the 'text' column.

IV. Remove EMojis
"""

def remove_emoji(txt):
  new=""
  for i in txt:
    if i.isascii(): #If i exist inside ascii value, toh hi use new string me daalna , vrna nahi
      new+=i
  return new

df['text']= df['text'].apply(remove_emoji)

"""V. Remove Stopwords (is , am ,are, etc)"""

import nltk

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('punkt') #it is used for tokenization
nltk.download('stopwords')

stop_words= set(stopwords.words('english'))
print(stop_words)
len(stop_words)

df.loc[1]['text']

"""###TOKENIZATION"""

def remove(txt):
  words=word_tokenize(txt)
  clean_txt=[]
  for i in words:
    if not i in stop_words:
      clean_txt.append(i)

  return ' '.join(clean_txt)

"""def remove(txt): — defines a function that takes one argument txt (a string).

words = word_tokenize(txt) — splits txt into tokens (words, punctuation) using word_tokenize (commonly from NLTK).

clean_txt = [] — creates an empty list to collect tokens that are not stop words.

for i in words: — iterates over each token i.

if not i in stop_words: — checks whether the token is not in the stop_words collection (assumed to be defined elsewhere).

clean_txt.append(i) — keeps the token when it’s not a stop word.

return ' '.join(clean_txt) — joins the kept tokens with spaces and returns the resulting string.
"""

import nltk
nltk.download('punkt_tab')
df['text']=df['text'].apply(remove)

import nltk
nltk.download('punkt_tab')
print("NLTK 'punkt_tab' resource downloaded successfully.")

df['text']=df['text'].apply(remove)
print("Stopwords removed from the 'text' column.")

df.loc[1]['text']
#now in ouput all the stop words are removed

from sklearn.model_selection import train_test_split

x=df['text']
y=df['emotion']

x_train, x_test, y_train, y_test= train_test_split(x,y,test_size=0.2, random_state=42)

"""Bag of Words"""

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer

bow_vectorizer=CountVectorizer()

x_train_bow=bow_vectorizer.fit_transform(x_train)
x_test_bow=bow_vectorizer.transform(x_test)

x_train_bow

x_test_bow

#Using Naive Bayes Model

from sklearn.naive_bayes import MultinomialNB#for  text classification we use MULTINOMIAL NAIVE BAYES

from sklearn.metrics import accuracy_score

NB_model=MultinomialNB()
NB_model.fit(x_train_bow, y_train)

NB_pred=NB_model.predict(x_test_bow)
accuracy_score(y_test, NB_pred)

NB_pred

y_test

"""###TF IDF"""

tfidf_vectorizer=TfidfVectorizer()
x_train_tfidf= tfidf_vectorizer.fit_transform(x_train)
x_test_tfidf= tfidf_vectorizer.transform(x_test)

NB2_model=MultinomialNB()
NB2_model.fit(x_train_tfidf, y_train)

NB2_pred=NB2_model.predict(x_test_tfidf)

accuracy_score(y_test, NB2_pred)

#USING LOGISTIC REGRESSION

from sklearn.linear_model import LogisticRegression

logistic_model=LogisticRegression(max_iter=1000)
logistic_model.fit(x_train_tfidf, y_train)

logistic_pred=logistic_model.predict(x_test_tfidf)

accuracy_score(y_test, logistic_pred)